#summary Frequently Asked Questions
#labels Deprecated

Frequently Asked Questions. For various values of "frequently".
<wiki:toc max_depth="3" />

== What do you mean by: What I think I know about scalability is wrong?!!? ==

If you think that 6 milliseconds to iterate a collection as seen in a trivial test or microbenchmark will be fine for your application, then unfortunately it's true. That is, if your application gets any reasonable amount of traffic, it will die, and it won't be graceful.

This question is best answered with another question:

*What is the average latency per request for a queue of 10 requests where each request takes 10ms to process?*

Here is a nice diagram:
[http://cqengine.googlecode.com/svn/wiki/images/latency-in-a-queue.png]

Overall latency per request, is processing time for that request, plus the sum of processing times for requests ahead of it in the queue. Or average latency = *_processing time_* + *_queueing delay_*.

Some interesting facts about this:
  * Average latency is 50ms
  * 50% requests take longer than 50ms to process
  * 10% of requests take 100ms to process

When designing applications, even looking at *average* response time alone is dangerous. From the diagram above it should be clear that _variability_ in response times is both likely, and problematic. If 50ms response time was deemed acceptable, what about 1 in 10 requests having latency of 100ms? The _distribution_ of latencies at various workloads or for spikes in traffic is important.

Anecdote: If you suspect that a particular database query is slowing down your application, but the DBA says its processing time is 10ms, the correct response is _"what's the queue length?"_.

Failure to appreciate _queueing delay_ is a major reason for sudden unresponsiveness of applications under high load: queues just start growing, latency skyrockets. Even in applications which do not have explicit queues, queue-like behaviour occurs behind any synchronized logic, blocking IO calls, or behind any processing step which cannot process requests faster than they are received.

A related culprit, is oversubscription of number of threads-to-CPU cores, on CPU-bound workloads (such as iteration). If you have 8 threads trying to iterate collections in parallel, and only 4 cores, latency per request will _more than_ double (it gets compounded by the overhead of _context-switching_). Tying up threads to process requests for longer, additionally compounds the problem by requiring more threads to be created in thread pools, to handle additional incoming requests.

The answer to this question really is:

*The results of microbenchmarks represent rarely achieved best-case scenarios from laboratory conditions*. 

Queueing delay and CPU starvation will multiply latencies in real applications.

This applies to the benchmarks in this project for both iteration and CQEngine. Those results will not be seen in the real world. But the remit of CQEngine, is to be faster in the first place, and to make latency a _tunable_ thing.