#summary Frequently Asked Questions
#labels Deprecated

Frequently Asked Questions. For various values of "frequently".
<wiki:toc max_depth="3" />

== What do you mean by: What I think I know about scalability is wrong?!!? ==

If you think that 6 milliseconds to iterate a collection will be fine for your application, then unfortunately it's true. That is, if your application gets any reasonable amount of traffic, it will die, and it won't be graceful.

This question is best answered with another question:

*What is the average latency per request for a queue of 10 requests where each request takes 10ms to process?*

Here is a nice diagram:
[http://cqengine.googlecode.com/svn/wiki/images/latency-in-a-queue.png]

Overall latency per request, is processing time for that request, plus the sum of processing times for requests in the queue ahead of it. Or _processing time_ + *_queueing delay_*.

Some interesting facts about this:
  * Average latency is 50ms
  * 50% requests take longer than 50ms to process
  * 10% of requests take 100ms to process

When designing applications, even looking at *average* response time alone is dangerous. From the diagram above it should be clear that _variability_ in response times is both likely, and problematic. If 50ms response time was deemed acceptable, what about 1 in 10 requests having latency of 100ms? The _distribution_ of latencies at various workloads is important.

Failure to appreciate _queueing delay_ is a major reason for the failure of applications under high load. Even in applications which do not have explicit queues, queues will form implicitly behind any synchronized code, any blocking IO call, or any part of the application which cannot process requests faster than they are received.

A related culprit, is oversubscription of number of threads-to-CPU cores, on CPU-bound workloads (such as iteration). If you have 8 threads trying to iterate collections in parallel, and only 4 cores, latency per request will _more than_ double (it gets compounded by the overhead of _context-switching_). Tying up threads to process requests for longer, additionally compounds the problem by requiring more threads to be created in thread pools, to handle additional incoming requests. Then when any `max_threads` threshold is reached on a thread pool, requests will be queued.

The answer to this question really is:

*The results of microbenchmarks represent rarely achieved best-case scenarios in laboratory conditions*.

This applies to the benchmarks in this project for both iteration and CQEngine. But the _purpose_ of CQEngine, is to make processing time _tunable_, so that applications can scale to higher workloads without additional hardware.